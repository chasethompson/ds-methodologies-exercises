{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Acquistion Exercises\n",
    "\n",
    "### Use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data.\n",
    "\n",
    "- print the first 3 rows\n",
    "- print the number of rows and columns (shape)\n",
    "- print the column names\n",
    "- print the data type of each column\n",
    "- print the summary statistics for each of the numeric variables. Would you recommend rescaling the data based on these statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "\n",
    "import env\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris data from seaborn\n",
    "# print the first 3 rows\n",
    "\n",
    "df_iris = sns.load_dataset('iris')\n",
    "df_iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows and columns (shape)\n",
    "\n",
    "df_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names\n",
    "\n",
    "df_iris.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way\n",
    "\n",
    "list(df_iris.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data type of each column\n",
    "\n",
    "df_iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary statistics for each of the numeric variables.\n",
    "# Would you recommend rescaling the data based on these statistics?\n",
    "\n",
    "df_iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wouldn't recommend rescaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Table1_CustDetails the excel module dataset, Excel_Exercises.xlsx, into a dataframe, df_excel\n",
    "\n",
    "- assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "- print the number of rows of your original dataframe\n",
    "- print the first 5 column names\n",
    "- print the column names that have a data type of object\n",
    "- compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custdetails = pd.read_excel('Spreadsheets_Exercises_Solutions.xlsx', sheet_name ='Table1_CustDetails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custdetails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "\n",
    "df_excel_sample = custdetails.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows of your original dataframe\n",
    "\n",
    "len(custdetails.index)\n",
    "\n",
    "print(f'My original DataFrame has {custdetails.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "\n",
    "custdetails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 column names\n",
    "\n",
    "custdetails.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names that have a data type of object\n",
    "\n",
    "custdetails.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "custdetails.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# as a loop\n",
    "\n",
    "for col in custdetails:\n",
    "    if custdetails[col].dtype == 'O':\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave the loops in the 80s and do it with a list comprehension!\n",
    "\n",
    "[col for col in custdetails if custdetails[col].dtype == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the range for each of the numeric variables\n",
    "\n",
    "monthly_charges = custdetails['monthly_charges']\n",
    "monthly_charges_range = monthly_charges.max() - monthly_charges.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_charges_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_charges = custdetails['total_charges']\n",
    "total_charges_range = total_charges.max() - total_charges.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_charges_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# another way to do it\n",
    "\n",
    "numeric_df = custdetails.select_dtypes(['int64', 'float64'])\n",
    "print(f'The range of each numeric columns is:')\n",
    "print('-------------------------------------')\n",
    "print(round(numeric_df.max() - numeric_df.min(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custdetails.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from this google sheet into a dataframe, df_google\n",
    "\n",
    "- print the first 3 rows\n",
    "- print the number of rows and columns\n",
    "- print the column names\n",
    "- print the data type of each column\n",
    "- print the summary statistics for each of the numeric variables\n",
    "- print the unique values for each of your categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from this google sheet into a dataframe, df_google\n",
    "\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "\n",
    "df_google = pd.read_csv(csv_export_url)\n",
    "\n",
    "# print the first 3 rows\n",
    "\n",
    "df_google.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows and columns\n",
    "\n",
    "df_google.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_google.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data type of each column\n",
    "\n",
    "df_google.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary statistics for each of the numeric variables\n",
    "\n",
    "df_google.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the unique values for each of your categorical variables\n",
    "\n",
    "df_google['Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google['Sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_google['Ticket'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google['Cabin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way\n",
    "print(df_google.Name.unique())\n",
    "print('----------------------------')\n",
    "print(df_google.Name.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_google['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_uniques(df):\n",
    "    for col in df:\n",
    "        if df[col].dtype == 'O':\n",
    "            print(col)\n",
    "            print('-------------')\n",
    "            print(df[col].value_counts(dropna=False))\n",
    "            print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_uniques(df_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a new python module, acquire.py:\n",
    "\n",
    "#### get_titanic_data: returns the titanic data from the codeup data science database as a pandas data frame.\n",
    "\n",
    "#### get_iris_data: returns the data from the iris_db on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the species_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_db = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Prep Exercises\n",
    "\n",
    "### The end product of this exercise should be the specified functions in a python script named prepare.py. Do these in your classification_exercises.ipynb first, then transfer to the prepare.py file.\n",
    "\n",
    "#### Iris Data\n",
    "\n",
    "- Use the function defined in acquire.py to load the iris data.\n",
    "- Drop the species_id and measurement_id columns.\n",
    "- Rename the species_name column to just species.\n",
    "- Encode the species name using a sklearn label encoder. Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "- Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the species_id and measurement_id columns.\n",
    "\n",
    "df = df.drop(columns=['species_id', 'measurement_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the species_name column to just species.\n",
    "\n",
    "df.rename(columns={'species_name':'species'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the species name using a sklearn label encoder.\n",
    "# Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "\n",
    "# inverse_transform(self, y) Transform labels back to original encoding.\n",
    "\n",
    "# Make the thing\n",
    "\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit the thing\n",
    "\n",
    "label_encoder.fit(df[['species']])\n",
    "\n",
    "# Transform the thing\n",
    "\n",
    "m = label_encoder.transform(df[['species']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.DataFrame(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(m)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0:'species_encoding'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function named prep_iris that accepts the untransformed iris data, and\n",
    "# returns the data with the transformations above applied.\n",
    "\n",
    "def encode_species(train, test):\n",
    "    encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "    encoder.fit(train[['species']])\n",
    "    \n",
    "    cols = ['species_' + c for c in encoder.categories_[0]]\n",
    "    \n",
    "    m = encoder.transform(train[['species']]).todense()\n",
    "\n",
    "    train = pd.concat([train,pd.DataFrame(m, columns=cols, index=train.index)], axis=1)\n",
    "    \n",
    "    m = encoder.transform(test[['species']]).todense()\n",
    "    \n",
    "    test = pd.concat([test,pd.DataFrame(m, columns=cols, index=test.index)], axis=1)\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "\n",
    "def prep_iris(df):\n",
    "    df = df.drop(columns=['species_id'])\n",
    "    df = df.rename(columns={'species_name':'species'})\n",
    "    train, test = sklearn.model_selection.train_test_split(df, train_size=.8, random_state=19)\n",
    "    train, test = encode_species(train, test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = prep_iris(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Data\n",
    "\n",
    "- Use the function you defined in acquire.py to load the titanic data set.\n",
    "- Handle the missing values in the embark_town and embarked columns.\n",
    "- Remove the deck column.\n",
    "- Use a label encoder to transform the embarked column.\n",
    "- Scale the age and fare columns using a min max scaler. Why might this be beneficial? When might you not want to do this?\n",
    "- Create a function named prep_titanic that accepts the untransformed titanic data, and returns the data with the transformations above applied.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.embark_town = df.embark_town.fillna(df.embark_town.value_counts().head(1).index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = sklearn.model_selection.train_test_split(df, train_size=.8, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the missing values in the embark_town and embarked columns.\n",
    "train.embark_town.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.embark_town = train.embark_town.fillna('Southampton')\n",
    "test.embark_town = test.embark_town.fillna('Southampton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train.embarked, train.embark_town)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.embark = train.embarked.fillna('Southampton')\n",
    "test.embark = test.embarked.fillna('Southampton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.embark_town.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the deck column.\n",
    "\n",
    "train = train.drop(columns='deck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = test.drop(columns='deck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a label encoder to transform the embarked column.\n",
    "\n",
    "train['embarked'] = train['embarked'].astype(str)\n",
    "test['embarked'] = test['embarked'].astype(str)\n",
    "# make the thing\n",
    "encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "\n",
    "# fit the thing\n",
    "encoder.fit(train[['embarked']])\n",
    "\n",
    "cols = ['embarked_' + c for c in encoder.categories_[0]]\n",
    "\n",
    "#transform the thing\n",
    "# .todense to convert from sparse matrix to plain old 2d numpy\n",
    "m = encoder.transform(train[['embarked']]).todense()\n",
    "\n",
    "train = pd.concat([\n",
    "    train,\n",
    "    pd.DataFrame(m, columns=cols, index=train.index)\n",
    "], axis=1)\n",
    "\n",
    "m = encoder.transform(test[['embarked']]).todense()\n",
    "\n",
    "test = pd.concat([\n",
    "    test,\n",
    "    pd.DataFrame(m, columns=cols, index=test.index)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    return df.drop(columns=\"deck\")\n",
    "\n",
    "def fill_na(df):\n",
    "    df.embark_town = df.embark_town.fillna('Southampton')\n",
    "    df.embarked = df.embarked.fillna('S')\n",
    "    return df\n",
    "    \n",
    "def encode_titanic(train, test):\n",
    "    encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "    encoder.fit(train[[\"embarked\"]])\n",
    "\n",
    "    m = encoder.transform(train[[\"embarked\"]]).todense()\n",
    "\n",
    "    train = pd.concat([train, pd.DataFrame(m, columns=encoder.categories_[0], index=train.index)], axis=1)\n",
    "\n",
    "    m = encoder.transform(test[[\"embarked\"]]).todense()\n",
    "\n",
    "    test = pd.concat([train, pd.DataFrame(m, columns=encoder.categories_[0], index=test.index)], axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def impute_titanic(train, test):\n",
    "    imputer = sklearn.impute.SimpleImputer(strategy='mean')\n",
    "    imputer.fit(train[['age']])\n",
    "    train.age = imputer.transform(train[['age']])\n",
    "    test.age = imputer.transform(test[['age']])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def scale_titanic(train, test):\n",
    "    train_scaled = train[['age', 'fare']]\n",
    "    test_scaled = test[['age', 'fare']]\n",
    "    scaler, train_scaled, test_scaled = split_scale.min_max_scaler(train_scaled, test_scaled)\n",
    "    return scaler, train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = sklearn.model_selection.train_test_split(df, random_state=19, train_size = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = encode_titanic(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, train, test = prepare.prep_titanic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_id</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.537918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0.293286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015216</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>595</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0.483555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047138</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0.796140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>887</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0.252514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058556</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     passenger_id  survived  pclass     sex       age  sibsp  parch      fare  \\\n",
       "263           263         0       1    male  0.537918      0      0  0.000000   \n",
       "395           395         0       3    male  0.293286      0      0  0.015216   \n",
       "595           595         0       3    male  0.483555      1      1  0.047138   \n",
       "94             94         0       3    male  0.796140      0      0  0.014151   \n",
       "887           887         1       1  female  0.252514      0      0  0.058556   \n",
       "\n",
       "    embarked  class  embark_town  alone    C    Q    S  \n",
       "263        S  First  Southampton      1  0.0  0.0  1.0  \n",
       "395        S  Third  Southampton      1  0.0  0.0  1.0  \n",
       "595        S  Third  Southampton      0  0.0  0.0  1.0  \n",
       "94         S  Third  Southampton      1  0.0  0.0  1.0  \n",
       "887        S  First  Southampton      1  0.0  0.0  1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
